# ğŸ¤– Quittungen mit lokalem LLM analysieren

## ğŸ¯ Ãœbersicht

Mit diesem Tool kannst du deine Testdaten aus der Datenbank mit dem **lokalen LLM** (Ollama) analysieren und Fragen dazu stellen.

## ğŸš€ Schnellstart

### 1. Ollama starten (falls nicht lÃ¤uft)

```bash
ollama serve
```

In einem **anderen Terminal**:

```bash
# Modelle installieren (falls noch nicht vorhanden)
ollama pull llama3.2
```

### 2. Chat starten

```bash
cd ai-backend
python3 chat_with_db_receipts.py
```

## ğŸ“‹ Verwendung

### Einfacher Chat-Modus

```bash
python3 chat_with_db_receipts.py
```

- LÃ¤dt alle Quittungen aus der Datenbank
- Formatiert sie als Kontext fÃ¼r das LLM
- Startet interaktiven Chat

### Mit RAG-Integration (empfohlen)

```bash
python3 chat_with_db_receipts.py --load-rag
```

- LÃ¤dt Quittungen auch in ChromaDB (RAG-System)
- ErmÃ¶glicht **semantische Suche**
- Bessere Kontext-Erkennung

### Limitierte Anzahl (fÃ¼r Tests)

```bash
python3 chat_with_db_receipts.py --limit 10
```

Nur die ersten 10 Quittungen laden.

## ğŸ’¬ Beispiel-Fragen

### Finanz-Fragen:

```
â“ Wie viel habe ich insgesamt ausgegeben?
â“ Was ist der Durchschnittsbetrag pro Quittung?
â“ Zeige mir meine Top-5 Ausgaben
â“ Wie viel fÃ¼r Kategorie "Meals"?
```

### Audit-Fragen:

```
â“ Welche Quittungen haben Rechenfehler?
â“ Zeige mir alle verdÃ¤chtigen Quittungen
â“ Gibt es Duplikate?
â“ Welche Quittungen fehlt die MwSt?
```

### Detail-Fragen:

```
â“ Was habe ich bei Deutsche Bahn gekauft?
â“ Zeige mir alle Alkohol-KÃ¤ufe
â“ Was waren die teuersten Quittungen?
â“ Wie viel habe ich in Oktober ausgegeben?
```

## ğŸ”§ Wie es funktioniert

### 1. Datenbank â†’ Kontext

```
SQLite DB
  â†“
Quittungen laden
  â†“
Als strukturierten Text formatieren
  â†“
An LLM als Kontext senden
```

### 2. LLM-Verarbeitung

```
Deine Frage
  â†“
LLM erhÃ¤lt:
  - Deine Frage
  - Alle Quittungen als Kontext
  - Chat-History
  â†“
Generiert natÃ¼rliche Antwort
```

### 3. RAG-Modus (--load-rag)

```
SQLite DB
  â†“
Quittungen â†’ RAG-Format konvertieren
  â†“
In ChromaDB speichern (Vektoren)
  â†“
Semantische Suche mÃ¶glich
  â†“
Nur relevante Quittungen als Kontext
```

## ğŸ“Š Was wird angezeigt?

### Beispiel-Output:

```
ğŸ’¬ CHAT MIT LOKALEM LLM
============================================================
ğŸ“Š 50 Quittungen geladen

â“ Deine Frage: Wie viel habe ich insgesamt ausgegeben?

ğŸ¤” Denke nach...

ğŸ¤– Antwort:
Basierend auf den Daten aus den 50 Quittungen habe ich 
einen Gesamtbetrag von 9.484,42 EUR ausgegeben.

Die durchschnittliche Quittung betrÃ¤gt 189,69 EUR.

Die Quittungen decken einen Zeitraum vom 31. August 2025 
bis zum 28. November 2025 ab.
------------------------------------------------------------
```

## ğŸ¯ Unterschiede: Mit vs. Ohne RAG

### Ohne RAG (`--load-rag` nicht verwendet):
- âœ… Alle Quittungen werden als Kontext gesendet
- âœ… LLM sieht komplette Daten
- âš ï¸  Bei vielen Quittungen â†’ lÃ¤ngerer Kontext
- âš ï¸  LLM muss selbst filtern/suchen

### Mit RAG (`--load-rag`):
- âœ… Semantische Suche findet relevante Quittungen
- âœ… Nur passende Quittungen als Kontext
- âœ… Schneller, prÃ¤ziser
- âœ… Bessere Ergebnisse bei spezifischen Fragen

## ğŸ› Troubleshooting

### Problem: "Ollama ist nicht verfÃ¼gbar"

**LÃ¶sung:**
```bash
# Terminal 1: Ollama starten
ollama serve

# Terminal 2: Modelle prÃ¼fen
ollama list

# Falls nÃ¶tig: Modell installieren
ollama pull llama3.2
```

### Problem: "Keine Quittungen gefunden"

**LÃ¶sung:**
```bash
# Testdaten generieren
cd backend
python3 seed.py
```

### Problem: LLM antwortet nicht richtig

**MÃ¶gliche Ursachen:**
1. **Falsches Modell:** Stelle sicher, dass `llama3.2` installiert ist
2. **Zu viele Quittungen:** Nutze `--limit 10` fÃ¼r Tests
3. **Unklare Frage:** Stelle spezifische Fragen

## ğŸ’¡ Tipps

### 1. Spezifische Fragen stellen

**Besser:**
- "Wie viel habe ich bei Deutsche Bahn ausgegeben?"
- "Zeige mir Quittungen mit Rechenfehlern"

**Weniger gut:**
- "Zeig mir Quittungen" (zu allgemein)
- "Was ist hier?" (unklar)

### 2. Schrittweise Fragen

```
1. "Wie viele Quittungen habe ich?"
2. "Wie viel insgesamt?"
3. "Was war die teuerste?"
```

### 3. Kombiniere mit Analyse-Tool

```bash
# Erst: Statistiken anzeigen
python3 analyze_receipts.py

# Dann: Spezifische Fragen stellen
python3 chat_with_db_receipts.py --load-rag
```

## ğŸ”„ Workflow-Beispiel

```bash
# 1. Testdaten generieren
cd backend && python3 seed.py

# 2. Statistiken anzeigen
cd ../ai-backend && python3 analyze_receipts.py

# 3. Ollama starten (Terminal 1)
ollama serve

# 4. Chat starten (Terminal 2)
cd ai-backend
python3 chat_with_db_receipts.py --load-rag

# 5. Fragen stellen
â“ Wie viel insgesamt?
â“ Zeige mir verdÃ¤chtige Quittungen
â“ Was war meine grÃ¶ÃŸte Ausgabe?
```

## ğŸ“š Weitere Ressourcen

- `ANALYSE_ANLEITUNG.md` - Datenbank-Analyse
- `INTEGRATION_PARTNER2.md` - API-Dokumentation
- `LLM_TEST_ANLEITUNG.md` - LLM-Testing Details

---

**Viel SpaÃŸ beim Chatten mit deinen Quittungen! ğŸ¤–ğŸ’¬**


