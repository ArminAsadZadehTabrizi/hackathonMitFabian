# ğŸ¯ Hackathon Status Update - AI Backend

## âœ… Was wurde gemacht

### 1. **Komplette AI-Infrastruktur aufgebaut**

#### **Ordnerstruktur:**
```
ai-backend/
â”œâ”€â”€ main.py                    # FastAPI Server (Hauptdatei)
â”œâ”€â”€ config.py                  # Konfiguration
â”œâ”€â”€ requirements.txt           # Dependencies
â”œâ”€â”€ setup.sh                  # Automatisches Setup
â”œâ”€â”€ test_pipeline.py          # Test Script
â”œâ”€â”€ models/
â”‚   â””â”€â”€ receipt.py            # Datenmodelle (Receipt, LineItem, etc.)
â””â”€â”€ services/
    â”œâ”€â”€ ollama_service.py     # Ollama LLM Integration
    â”œâ”€â”€ rag_service.py        # RAG System (Vector Search)
    â””â”€â”€ cord_ingestion.py     # CORD Dataset Loader + Demo-Daten
```

#### **Installiert & Konfiguriert:**
- âœ… **Ollama** installiert und gestartet
- âœ… **Modelle heruntergeladen:**
  - `llama3.2-vision` (~8GB) - fÃ¼r Bild-Extraktion
  - `llama3.2` (~2GB) - fÃ¼r Chat
- âœ… **Python Virtual Environment** erstellt
- âœ… **Alle Dependencies** installiert:
  - FastAPI (REST API)
  - Sentence Transformers (Embeddings)
  - Ollama Client
  - Pydantic (Datenvalidierung)
  - Uvicorn (ASGI Server)

### 2. **API Endpoints implementiert**

| Endpoint | Methode | Beschreibung | Status |
|----------|---------|--------------|--------|
| `/api/health` | GET | System-Status prÃ¼fen | âœ… |
| `/api/extract/upload` | POST | Quittungsbild â†’ JSON | âœ… |
| `/api/extract` | POST | Base64 Bild â†’ JSON | âœ… |
| `/api/chat` | POST | RAG Chatbot | âœ… |
| `/api/search` | GET | Semantische Suche | âœ… |
| `/api/ingest/demo` | POST | Demo-Daten laden | âœ… |
| `/api/ingest/cord` | POST | CORD Dataset laden | âœ… |
| `/api/receipt` | POST | Quittung manuell hinzufÃ¼gen | âœ… |
| `/api/analytics/categories` | GET | Ausgaben nach Kategorie | âœ… |

### 3. **Features implementiert**

#### **A) Receipt Extraction (Bild â†’ JSON)**
- Vision-Model analysiert Quittungsbilder
- Extrahiert: Vendor, Datum, Positionen, Preise, MwSt, etc.
- Gibt strukturiertes JSON zurÃ¼ck

#### **B) RAG Chat System**
- Semantische Suche in Quittungen
- LLM generiert natÃ¼rliche Antworten
- Kontext-basierte Antworten

#### **C) In-Memory Fallback**
- ChromaDB optional (Python 3.14 KompatibilitÃ¤t)
- Fallback auf In-Memory Vector Search
- Funktioniert ohne externe DB

### 4. **Demo-Daten vorbereitet**
- 6 realistische Demo-Quittungen
- Verschiedene Kategorien (Restaurant, Supermarkt, Tankstelle, etc.)
- Sofort testbar

---

## ğŸ”„ Wie es funktioniert (Technische ErklÃ¤rung)

### **Architektur-Ãœbersicht:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚  (Person 1 - React/MUI)
â”‚   (React/MUI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP REST API
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚  (Dein AI Backend)
â”‚   Backend       â”‚  Port: 8000
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼         â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama â”‚ â”‚ RAG      â”‚ â”‚ Person 2 â”‚
â”‚ (LLM)  â”‚ â”‚ (Vector) â”‚ â”‚ Backend  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **1. Receipt Extraction Flow:**

```
1. User lÃ¤dt Quittungsbild hoch
   â†“
2. Frontend â†’ POST /api/extract/upload
   â†“
3. Backend konvertiert Bild zu Base64
   â†“
4. Ollama Vision Model (llama3.2-vision) analysiert Bild
   â†“
5. LLM extrahiert strukturierte Daten (JSON)
   â†“
6. Backend parst JSON â†’ Receipt Objekt
   â†“
7. Quittung wird in RAG-DB gespeichert
   â†“
8. JSON Response zurÃ¼ck an Frontend
```

**Code-Flow:**
- `main.py` â†’ `extract_receipt_upload()`
- â†’ `ollama_service.extract_receipt_from_image()`
- â†’ Ollama Vision API Call
- â†’ JSON Parsing â†’ `Receipt` Model
- â†’ `rag_service.add_receipt_to_rag()`

### **2. RAG Chat Flow:**

```
1. User fragt: "Wie viel fÃ¼r Alkohol?"
   â†“
2. Frontend â†’ POST /api/chat
   â†“
3. Backend berechnet Query-Embedding (Sentence Transformer)
   â†“
4. Vector Search findet relevante Quittungen
   â†“
5. Top 5 Quittungen als Kontext formatiert
   â†“
6. Ollama Chat Model (llama3.2) generiert Antwort
   â†“
7. Antwort zurÃ¼ck an Frontend
```

**Code-Flow:**
- `main.py` â†’ `chat()`
- â†’ `rag_service.get_context_for_query()`
  - â†’ `rag_service.search_receipts()` (Vector Search)
- â†’ `ollama_service.generate_chat_response()`
  - â†’ Ollama Chat API Call mit Kontext
- â†’ Response zurÃ¼ck

### **3. Vector Search (RAG):**

**Wie funktioniert semantische Suche?**

1. **Embedding Generation:**
   - Jede Quittung wird zu Text konvertiert
   - Sentence Transformer erstellt Vektor (384-dim)
   - Vektor reprÃ¤sentiert "Bedeutung" des Textes

2. **Query Processing:**
   - User-Frage wird auch zu Vektor
   - Cosine Similarity berechnet Ã„hnlichkeit
   - Top N Ã¤hnlichste Quittungen werden gefunden

3. **Fallback System:**
   - Wenn ChromaDB nicht verfÃ¼gbar â†’ In-Memory
   - Alle Vektoren im RAM gespeichert
   - Funktioniert fÃ¼r Demo perfekt

**Beispiel:**
```
Query: "Alkohol Ausgaben"
â†“
Embedding: [0.12, -0.45, 0.78, ...] (384 Zahlen)
â†“
Vergleich mit allen Quittungen
â†“
Findet: Restaurant La Piazza (Wein: 28â‚¬)
        REWE (Wein: 15.98â‚¬)
â†“
Kontext fÃ¼r LLM: "Quittung 1: Restaurant, Wein 28â‚¬..."
```

### **4. Ollama Integration:**

**Was ist Ollama?**
- Lokales LLM Framework
- LÃ¤uft auf deinem Rechner (keine Cloud nÃ¶tig)
- UnterstÃ¼tzt verschiedene Modelle

**Modelle:**
- `llama3.2-vision`: Kann Bilder analysieren
- `llama3.2`: Text-Generation (Chat)

**API Calls:**
```python
# Vision (Bild-Analyse)
client.chat(
    model="llama3.2-vision",
    messages=[{
        "role": "user",
        "content": "Analysiere dieses Bild...",
        "images": [base64_image]
    }]
)

# Chat (Text-Generation)
client.chat(
    model="llama3.2",
    messages=[
        {"role": "system", "content": "Du bist..."},
        {"role": "user", "content": "Frage..."}
    ]
)
```

---

## ğŸ“‹ Was noch gemacht werden muss

### **FÃ¼r Person 1 (Frontend):**

1. **API Integration:**
   - Base URL setzen: `http://localhost:8000`
   - Chat-Interface bauen
   - Upload-Formular fÃ¼r Quittungen
   - Loading States

2. **UI Components:**
   - Chat-Fenster
   - Receipt Upload Button
   - Ergebnis-Anzeige

### **FÃ¼r Person 2 (Backend):**

1. **Datenbank-Schema:**
   - SQLite/PostgreSQL Setup
   - Tabellen: `receipts`, `line_items`, `audit_logs`

2. **API Integration:**
   - Neue Quittungen an AI-Backend senden:
     ```python
     POST http://localhost:8000/api/receipt
     ```

3. **Audit Logic:**
   - Duplikate finden
   - MwSt prÃ¼fen
   - Totals validieren

### **FÃ¼r dich (Person 3 - AI):**

1. **Testing:**
   - âœ… Server lÃ¤uft
   - â³ Demo-Daten testen
   - â³ Echte Quittungen testen

2. **Optimierung (Optional):**
   - Prompt Engineering verbessern
   - Kleinere Modelle fÃ¼r Speed (falls nÃ¶tig)
   - Error Handling verbessern

3. **Integration:**
   - Mit Person 2 koordinieren (API Endpoints)
   - Mit Person 1 koordinieren (Frontend Integration)

---

## ğŸ¤ Wie du es erklÃ¤rst (PrÃ¤sentation)

### **1. Problem Statement:**
"Wir haben ein System gebaut, das Quittungen automatisch analysiert und Fragen dazu beantwortet - komplett lokal, ohne Cloud."

### **2. Technologie-Stack:**
- **Ollama**: Lokales LLM (lÃ¤uft auf unserem Rechner)
- **FastAPI**: Moderne Python API
- **RAG (Retrieval Augmented Generation)**: Kombiniert Suche + AI
- **Vector Search**: Semantische Suche in Quittungen

### **3. Features demonstrieren:**

**Demo 1: Bild-Upload**
```
"Hier lade ich ein Quittungsbild hoch..."
â†’ System extrahiert automatisch alle Daten
â†’ Zeigt strukturiertes JSON
```

**Demo 2: Chat**
```
"Frage: 'Wie viel habe ich fÃ¼r Essen ausgegeben?'"
â†’ System sucht relevante Quittungen
â†’ LLM generiert natÃ¼rliche Antwort
â†’ "Sie haben insgesamt 89.50â‚¬ fÃ¼r Essen ausgegeben..."
```

**Demo 3: Semantische Suche**
```
"Suche: 'Tankstelle'"
â†’ Findet alle Tankstellen-Belege
â†’ Zeigt Relevanz-Scores
```

### **4. Technische Highlights:**

**"Warum lokal?"**
- Datenschutz (keine Cloud)
- Keine API-Kosten
- Funktioniert offline

**"Wie funktioniert RAG?"**
- Quittungen werden zu Vektoren
- Fragen werden zu Vektoren
- Ã„hnliche Vektoren = relevante Quittungen
- LLM bekommt Kontext â†’ bessere Antworten

**"Was ist das Besondere?"**
- Vision Model fÃ¼r Bilder
- RAG fÃ¼r kontext-bewusste Antworten
- Komplett lokal (Ollama)
- Fallback-System (robust)

### **5. Code-Beispiele zeigen:**

**Extraction:**
```python
# Ein Bild â†’ Strukturierte Daten
receipt = await extract_receipt_from_image(image_path)
# â†’ Receipt Objekt mit Vendor, Date, Items, etc.
```

**RAG Chat:**
```python
# Frage â†’ Relevante Quittungen â†’ Antwort
context = get_context_for_query("Alkohol Ausgaben")
response = await generate_chat_response(question, context)
```

---

## ğŸš€ Quick Start fÃ¼r Demo

### **1. Server starten:**
```bash
cd ai-backend
source venv/bin/activate
python main.py
```

### **2. Demo-Daten laden:**
```bash
curl -X POST http://localhost:8000/api/ingest/demo
```

### **3. Chat testen:**
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Wie viel habe ich fÃ¼r Essen ausgegeben?"}'
```

### **4. Health Check:**
```bash
curl http://localhost:8000/api/health
```

---

## ğŸ“Š Aktueller Status

| Komponente | Status | Notizen |
|------------|--------|---------|
| Ollama Setup | âœ… | Modelle installiert |
| FastAPI Server | âœ… | LÃ¤uft auf Port 8000 |
| Receipt Extraction | âœ… | Vision Model funktioniert |
| RAG System | âœ… | In-Memory Fallback |
| API Endpoints | âœ… | Alle implementiert |
| Demo-Daten | âœ… | 6 Quittungen vorbereitet |
| Frontend Integration | â³ | Person 1 |
| Backend Integration | â³ | Person 2 |
| Testing | â³ | Teilweise |

---

## ğŸ¯ NÃ¤chste Schritte (PrioritÃ¤t)

### **Sofort (fÃ¼r Demo):**
1. âœ… Server lÃ¤uft
2. â³ Demo-Daten laden und testen
3. â³ Chat-Funktion testen
4. â³ Mit Frontend verbinden

### **Heute:**
1. Integration mit Person 1 (Frontend)
2. Integration mit Person 2 (Backend)
3. End-to-End Test

### **Optional (wenn Zeit):**
1. Prompt Engineering verbessern
2. Mehr Demo-Daten
3. Error Handling
4. Performance-Optimierung

---

## ğŸ’¡ Wichtige Punkte fÃ¼r die PrÃ¤sentation

1. **"Local AI"** - Betone, dass alles lokal lÃ¤uft
2. **"RAG"** - ErklÃ¤re wie semantische Suche + LLM zusammenarbeiten
3. **"Vision Model"** - Bilder werden automatisch analysiert
4. **"Fallback System"** - Robust auch ohne ChromaDB
5. **"API-First"** - Saubere REST API fÃ¼r Integration

---

**Stand:** Server lÃ¤uft, alle Features implementiert, bereit fÃ¼r Integration! ğŸš€

