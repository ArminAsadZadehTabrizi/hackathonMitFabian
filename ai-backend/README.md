# ğŸ§  Finanz AI Backend

**Local LLM + RAG System fÃ¼r Quittungs-Analyse**

Dieses Backend nutzt:
- **Ollama** (lokales LLM) fÃ¼r Bild-Extraktion und Chat
- **ChromaDB** (Vektor-DB) fÃ¼r semantische Suche
- **FastAPI** fÃ¼r die REST API

---

## ğŸš€ Quick Start

### 1. Ollama installieren

```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# Download von https://ollama.com/download
```

### 2. Ollama starten

```bash
ollama serve
```

### 3. Setup ausfÃ¼hren

```bash
cd ai-backend
chmod +x setup.sh
./setup.sh
```

### 4. Server starten

```bash
source venv/bin/activate
python main.py
```

Der Server lÃ¤uft dann auf `http://localhost:8000`

---

## ğŸ“¡ API Endpoints

### Health Check
```bash
GET /api/health
```

### Quittung extrahieren (Bild â†’ JSON)
```bash
POST /api/extract/upload
Content-Type: multipart/form-data

file: <receipt_image.jpg>
```

### Chat mit RAG
```bash
POST /api/chat
Content-Type: application/json

{
    "message": "Wie viel habe ich fÃ¼r Essen ausgegeben?",
    "history": []
}
```

### Semantische Suche
```bash
GET /api/search?query=Tankstelle&limit=5
```

### Demo-Daten laden
```bash
POST /api/ingest/demo
```

---

## ğŸ”§ Konfiguration

Alle Einstellungen in `.env`:

```env
# Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2-vision      # FÃ¼r Bild-Analyse
OLLAMA_CHAT_MODEL=llama3.2        # FÃ¼r Chat

# ChromaDB
CHROMA_PERSIST_DIR=./chroma_db

# API
API_HOST=0.0.0.0
API_PORT=8000
```

---

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚â”€â”€â”€â”€â–¶â”‚   FastAPI       â”‚
â”‚   (React/MUI)   â”‚     â”‚   Backend       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼            â–¼            â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Ollama     â”‚ â”‚ ChromaDB â”‚ â”‚ SQLite   â”‚
           â”‚   (LLM)      â”‚ â”‚ (Vector) â”‚ â”‚ (Person2)â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Ordnerstruktur

```
ai-backend/
â”œâ”€â”€ main.py              # FastAPI Server
â”œâ”€â”€ config.py            # Konfiguration
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ setup.sh            # Setup Script
â”œâ”€â”€ models/
â”‚   â””â”€â”€ receipt.py      # Pydantic Models
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ollama_service.py    # LLM Integration
â”‚   â”œâ”€â”€ rag_service.py       # ChromaDB RAG
â”‚   â””â”€â”€ cord_ingestion.py    # Dataset Loader
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cord/           # CORD Dataset (optional)
â””â”€â”€ chroma_db/          # Vektor-Datenbank
```

---

## ğŸ¯ Features

### 1. Receipt Extraction (Bild â†’ JSON)
Das Vision-Model analysiert Quittungsbilder und extrahiert:
- Vendor Name & Adresse
- Datum
- Einzelpositionen mit Preisen
- Gesamtbetrag, MwSt, etc.

### 2. RAG Chat
Nutzer kÃ¶nnen Fragen stellen wie:
- "Wie viel habe ich fÃ¼r Alkohol ausgegeben?"
- "Zeige mir alle Restaurant-Belege"
- "Was waren meine Top 5 Ausgaben?"

Das System:
1. Sucht relevante Quittungen via Vector Search
2. Ãœbergibt diese als Kontext an das LLM
3. Generiert eine natÃ¼rlichsprachliche Antwort

### 3. CORD Dataset Integration
UnterstÃ¼tzt das CORD Dataset fÃ¼r Demo-Daten:
- https://github.com/clovaai/cord

---

## ğŸ¤ Integration mit Person 2 (Backend)

Person 2 sollte diese Endpoints aufrufen:

```python
import requests

# Neue Quittung in RAG speichern
requests.post("http://localhost:8000/api/receipt", json={
    "vendor_name": "REWE",
    "date": "2024-01-15",
    "total": 47.89,
    "category": "Supermarkt",
    "line_items": [...]
})

# Chat-Anfrage weiterleiten
response = requests.post("http://localhost:8000/api/chat", json={
    "message": "Wie viel fÃ¼r Essen?",
    "history": []
})
```

---

## ğŸ› Troubleshooting

### Ollama lÃ¤uft nicht
```bash
# PrÃ¼fen
curl http://localhost:11434/api/tags

# Neu starten
ollama serve
```

### Modell nicht gefunden
```bash
# Modelle auflisten
ollama list

# Modell laden
ollama pull llama3.2-vision
ollama pull llama3.2
```

### ChromaDB Fehler
```bash
# Datenbank lÃ¶schen und neu starten
rm -rf chroma_db/
python main.py
```

---

## ğŸ“Š Performance Tipps

1. **Kleinere Modelle nutzen** fÃ¼r schnellere Inference:
   - `llama3.2:1b` statt `llama3.2` fÃ¼r Chat
   - `llava:7b` statt `llama3.2-vision` fÃ¼r Bilder

2. **Batch Processing** fÃ¼r viele Quittungen:
   ```python
   from services.rag_service import add_receipts_batch
   add_receipts_batch([(id1, r1), (id2, r2), ...])
   ```

3. **GPU nutzen** (falls verfÃ¼gbar):
   Ollama nutzt automatisch die GPU wenn vorhanden.

---

Made with ğŸ§  fÃ¼r den Hackathon!


